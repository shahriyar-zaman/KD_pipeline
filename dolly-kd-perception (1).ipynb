{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth\n!pip install datasets transformers accelerate einops bitsandbytes trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:28:30.522111Z","iopub.execute_input":"2025-01-03T03:28:30.522561Z","iopub.status.idle":"2025-01-03T03:31:39.608397Z","shell.execute_reply.started":"2025-01-03T03:28:30.522523Z","shell.execute_reply":"2025-01-03T03:31:39.607167Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:31:39.609811Z","iopub.execute_input":"2025-01-03T03:31:39.610112Z","iopub.status.idle":"2025-01-03T03:31:56.208087Z","shell.execute_reply.started":"2025-01-03T03:31:39.610090Z","shell.execute_reply":"2025-01-03T03:31:56.207174Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"################################################################\n# 1) Load the TEACHER (frozen)\n################################################################\nteacher_max_seq_length = 2048 \nteacher_dtype = None           # Auto-detect dtype\nteacher_load_in_4bit = True    # 4bit quant for memory savings\n\nteacher_model, teacher_tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/SmolLM2-1.7B-Instruct\",\n    max_seq_length=teacher_max_seq_length,\n    dtype=teacher_dtype,\n    load_in_4bit=teacher_load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:31:56.209795Z","iopub.execute_input":"2025-01-03T03:31:56.210019Z","iopub.status.idle":"2025-01-03T03:32:13.327574Z","shell.execute_reply.started":"2025-01-03T03:31:56.210000Z","shell.execute_reply":"2025-01-03T03:32:13.326855Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57a687071952405592640608bd650add"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8479c992f33549918dad0acc0adfa151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/4.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae21462beab847b79f0b4870a776779a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac8505a242964014ae5f9727fa4b13d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b762b7179915404fa8525f0203104561"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ef7349dd98d44628bd77a4fe548de69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/423 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"036f67b33fbb4aaea95741090de499aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3defb9e092964dbf9a6157aca387e191"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Freeze teacher weights to avoid training them\nteacher_model.eval()\nfor param in teacher_model.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:32:13.328598Z","iopub.execute_input":"2025-01-03T03:32:13.328820Z","iopub.status.idle":"2025-01-03T03:32:13.334580Z","shell.execute_reply.started":"2025-01-03T03:32:13.328790Z","shell.execute_reply":"2025-01-03T03:32:13.333918Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"################################################################\n# 2) Load the STUDENT (with LoRA)\n################################################################\nstudent_max_seq_length = 2048\nstudent_dtype = None            # Auto-detect dtype\nstudent_load_in_4bit = True     # 4bit quant for memory savings\n\nstudent_model, student_tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/SmolLM2-135M-Instruct\",\n    max_seq_length=student_max_seq_length,\n    dtype=student_dtype,\n    load_in_4bit=student_load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:32:13.335363Z","iopub.execute_input":"2025-01-03T03:32:13.335561Z","iopub.status.idle":"2025-01-03T03:32:20.414932Z","shell.execute_reply.started":"2025-01-03T03:32:13.335543Z","shell.execute_reply":"2025-01-03T03:32:20.413935Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f14da515d074493894f8075bf6e1af26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eb956b02035429d82ec0b89282f6c53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6aeed579bcd4cdba2a45b531daa4994"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6502ee42efeb418eaf85e43fcdd17095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ed781901a5f4f68b6fa51dcf004d832"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0011f73ff9de4191b59bcf5063ad6abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/423 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b787d35ef6004945aa119def96baaff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de01395daf544d2389364cbf25f98444"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Apply LoRA to the student\nstudent_model = FastLanguageModel.get_peft_model(\n    student_model,\n    r=16,\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\", \n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:32:20.415840Z","iopub.execute_input":"2025-01-03T03:32:20.416130Z","iopub.status.idle":"2025-01-03T03:32:25.725214Z","shell.execute_reply.started":"2025-01-03T03:32:20.416096Z","shell.execute_reply":"2025-01-03T03:32:25.724288Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2024.12.12 patched 30 layers with 30 QKV layers, 30 O layers and 30 MLP layers.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"################################################################\n# 3) Prepare the Prompt Template\n################################################################\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, \npaired with an input that provides further context. \nWrite a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = student_tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"context\"]\n    outputs      = examples[\"response\"]\n    texts = []\n    for instr, inp, outp in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instr, inp, outp)\n        # Add EOS to ensure generation terminates\n        text += EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n################################################################\n# 4) Load the Alpaca Dataset & Apply Prompt-Formatting\n################################################################\n# Load and split the dataset\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n\n# Split dataset into training and validation sets\nfrom datasets import Dataset\n\n# Split dataset into training and validation sets\ndataset = dataset.train_test_split(test_size=0.1, seed=42)\ntrain_dataset = dataset['train']\neval_dataset = dataset['test']\n\n# Ensure test set has exactly 1000 entries without shuffling\n# eval_dataset = (eval_dataset.select(range(1000)))\n\n# Apply formatting to both datasets\ntrain_dataset = train_dataset.map(formatting_prompts_func, batched=True)\neval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:32:25.726060Z","iopub.execute_input":"2025-01-03T03:32:25.726378Z","iopub.status.idle":"2025-01-03T03:32:27.918701Z","shell.execute_reply.started":"2025-01-03T03:32:25.726347Z","shell.execute_reply":"2025-01-03T03:32:27.917800Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e8708cb806d4b078a7e08488111dde5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d5ce07257dd4a1c935b79867d9e08f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9495bd41e4e2414fa86ae6e32f178598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13509 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a2103e52ef84bd59fe6f6a524805c59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1502 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d59d06ddbb47e299d994c4b39719c6"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import torch.nn as nn\n\ndef perception(logits, epsilon=1e-5, dim=1):\n    \"\"\"\n    Perform perception on logits.\n\n    Parameters:\n    logits (torch.Tensor): A tensor of shape (B, N) or (B, S, V).\n    epsilon (float): A small constant to avoid division by zero in normalization.\n    dim (int): The dimension along which to compute the mean and variance.\n\n    Returns:\n    torch.Tensor: Normalized logits along the specified dimension.\n    \"\"\"\n    batch_mean = torch.mean(logits, dim=dim, keepdim=True)\n    batch_var = torch.var(logits, dim=dim, keepdim=True, unbiased=False)\n    x_normalized = (logits - batch_mean) / torch.sqrt(batch_var + epsilon)\n    return x_normalized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:32:27.920916Z","iopub.execute_input":"2025-01-03T03:32:27.921156Z","iopub.status.idle":"2025-01-03T03:32:27.925404Z","shell.execute_reply.started":"2025-01-03T03:32:27.921135Z","shell.execute_reply":"2025-01-03T03:32:27.924524Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"###############################################################\n# 4) Distillation Trainer\n###############################################################\nclass DistillationTrainer(SFTTrainer):\n    def __init__(\n        self,\n        teacher_model,\n        alpha=0.5,       # weight for dataset CE\n        temperature=1.0, # teacher distribution softening\n        *args, \n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n        self.alpha = alpha\n        self.temperature = temperature\n        \n        # Freeze teacher\n        self.teacher_model.eval()\n        for p in self.teacher_model.parameters():\n            p.requires_grad = False\n\n        # We'll manually compute CE, so define a suitable loss\n        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n\n    # # Accept **kwargs to handle extra unsloth arguments like num_items_in_batch\n    # def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n    #     # We'll do forward pass on student WITHOUT labels to ensure we get `logits`\n    #     student_out = model(\n    #         input_ids=inputs[\"input_ids\"],\n    #         attention_mask=inputs[\"attention_mask\"],\n    #     )\n    #     student_logits = student_out.logits  # shape (batch, seq_len, vocab_size)\n\n    #     # Manually compute cross-entropy against the known labels\n    #     labels = inputs[\"labels\"]  # shape (batch, seq_len)\n    #     ce_loss = self.loss_fct(\n    #         student_logits.view(-1, student_logits.size(-1)),\n    #         labels.view(-1)\n    #     )\n\n    #     # Teacher forward pass\n    #     with torch.no_grad():\n    #         teacher_out = self.teacher_model(\n    #             input_ids=inputs[\"input_ids\"],\n    #             attention_mask=inputs[\"attention_mask\"],\n    #         )\n    #         teacher_logits = teacher_out.logits\n\n    #     # Build a valid mask to ignore label=-100 tokens\n    #     valid_mask = labels.ne(-100)\n\n    #     # Student + teacher distributions\n    #     student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n    #     teacher_probs     = F.softmax(teacher_logits / self.temperature, dim=-1)\n\n    #     # Filter out invalid tokens\n    #     student_log_probs = student_log_probs[valid_mask]\n    #     teacher_probs     = teacher_probs[valid_mask]\n\n    #     # KL Divergence\n    #     kl_loss = F.kl_div(\n    #         student_log_probs,\n    #         teacher_probs,\n    #         reduction=\"batchmean\"\n    #     )\n\n    #     # Combine\n    #     loss = self.alpha * ce_loss + (1 - self.alpha) * (self.temperature**2) * kl_loss\n    #     return (loss, student_out) if return_outputs else loss\n\ndef compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n    labels = inputs[\"labels\"]\n\n    # 1) FORWARD PASS *without* labels\n    student_out = model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        # remove labels=labels here\n    )\n    student_logits = student_out.logits  # No longer None!\n\n    # 2) Manually compute CE\n    ce_loss = self.loss_fct(\n        student_logits.view(-1, student_logits.size(-1)),\n        labels.view(-1),\n    )\n\n    # 3) Teacher pass\n    with torch.no_grad():\n        teacher_out = self.teacher_model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n        )\n        teacher_logits = teacher_out.logits\n\n    # Apply perception with dim=1 (normalizing across the sequence dimension)\n    student_logits = perception(student_logits, dim=1)\n    teacher_logits = perception(teacher_logits, dim=1)\n\n    # 4) KD portion: apply mask for label=-100 tokens, compute KL, combine\n    valid_mask = labels.ne(-100)\n    student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n    teacher_probs     = F.softmax(teacher_logits / self.temperature, dim=-1)\n    student_log_probs = student_log_probs[valid_mask]\n    teacher_probs     = teacher_probs[valid_mask]\n    kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction=\"batchmean\")\n\n    # 5) Final distillation loss\n    loss = self.alpha * ce_loss + (1 - self.alpha) * (self.temperature ** 2) * kl_loss\n    return (loss, student_out) if return_outputs else loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:32:27.926933Z","iopub.execute_input":"2025-01-03T03:32:27.927258Z","iopub.status.idle":"2025-01-03T03:32:28.026698Z","shell.execute_reply.started":"2025-01-03T03:32:27.927210Z","shell.execute_reply":"2025-01-03T03:32:28.025783Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"################################################################\n# 6) Set Up TrainingArguments & DistillationTrainer\n################################################################\ntrain_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=60,               # or use num_train_epochs=1 for a full epoch\n    learning_rate=2e-4,\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"distilled_outputs\",\n    report_to=\"none\",  # or \"wandb\", \"tensorboard\", etc.\n)\n\ntrainer = DistillationTrainer(\n    teacher_model=teacher_model,\n    alpha=0.5,           # how much to weigh the ground-truth CE vs. teacher KL\n    temperature=1.0,     # if you need a \"softer\" teacher distribution, raise T\n    model=student_model,\n    tokenizer=student_tokenizer,\n    train_dataset=train_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=student_max_seq_length,\n    dataset_num_proc=2,\n    packing=False,\n    args=train_args,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:32:28.027601Z","iopub.execute_input":"2025-01-03T03:32:28.027942Z","iopub.status.idle":"2025-01-03T03:32:34.621530Z","shell.execute_reply.started":"2025-01-03T03:32:28.027913Z","shell.execute_reply":"2025-01-03T03:32:34.620476Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/13509 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ac21b5d57a4cc982ed1d396377eea2"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"################################################################\n# 7) Launch Distillation\n################################################################\ntrainer_stats = trainer.train()\nprint(trainer_stats)\n\n################################################################\n# 8) (Optional) Save or Push Your LoRA Adapters\n################################################################\n# student_model.save_pretrained(\"my_distilled_student_lora\")\n# student_model.push_to_hub(\"myusername/SmolLM2-135M-distilled-lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:32:34.622569Z","iopub.execute_input":"2025-01-03T03:32:34.622839Z","iopub.status.idle":"2025-01-03T03:34:26.179355Z","shell.execute_reply.started":"2025-01-03T03:32:34.622810Z","shell.execute_reply":"2025-01-03T03:34:26.178446Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 13,509 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 4,884,480\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 01:36, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.974700</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.724000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.942800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.572900</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.083900</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.913800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.472200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.741100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>3.007000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.719100</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.390600</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.239200</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.657300</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.830400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.652400</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.399700</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2.656100</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>2.482100</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.796600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.873400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>2.466600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>2.651300</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>2.497500</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.224200</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.443400</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>2.402400</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>2.539100</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>2.180400</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>2.276600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.886000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>2.454900</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>2.157100</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>2.201300</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>2.314200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>2.322500</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>2.051600</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>2.156900</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>2.243500</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>2.288800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.522200</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>2.890900</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.966700</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>2.038500</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>2.481000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.821800</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>2.165600</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>2.042400</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>1.826600</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>2.024000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.813200</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>1.958700</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.827500</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>2.211200</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>2.164400</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.957200</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>1.934400</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.472300</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>1.695000</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>2.599800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.091700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"TrainOutput(global_step=60, training_loss=2.339881632725398, metrics={'train_runtime': 108.5745, 'train_samples_per_second': 4.421, 'train_steps_per_second': 0.553, 'total_flos': 107089659007488.0, 'train_loss': 2.339881632725398, 'epoch': 0.03552923760177646})\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"################################################################\n# 7) (OPTIONAL) INFERENCE EXAMPLE\n################################################################\n# After training, let's enable 2x faster inference with Unsloth\nFastLanguageModel.for_inference(student_model)  # Speed optimization\n\n# We'll create a prompt using the same alpaca_prompt format\n# but we'll leave the \"output\" blank so the model can generate.\n\ninference_text = alpaca_prompt.format(\n    \"who am  i?\",  # instruction\n    \"\",  # context\n    \"\"   # response left blank for generation\n)\n\ninputs = student_tokenizer([inference_text], return_tensors=\"pt\").to(\"cuda\")\n\n# Use Hugging Face's TextStreamer to see the live output tokens\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(student_tokenizer)\n\n# Generate up to 128 new tokens from the student model\n_ = student_model.generate(\n    **inputs,\n    streamer=text_streamer,\n    max_new_tokens=128,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:34:26.180313Z","iopub.execute_input":"2025-01-03T03:34:26.180661Z","iopub.status.idle":"2025-01-03T03:34:33.360507Z","shell.execute_reply.started":"2025-01-03T03:34:26.180627Z","shell.execute_reply":"2025-01-03T03:34:33.359826Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, \npaired with an input that provides further context. \nWrite a response that appropriately completes the request.\n\n### Instruction:\nwho am  i?\n\n### Input:\n\n\n### Response:\n\nI am a 25-year-old male, born in the United States, who is from a small town in the Midwest. I have a family of 10 children, and I have a 10-year-old son who is 10 years old. I have a 10-year-old daughter who is 10 years old. I have a 10-year-old son who is 10 years old. I have a 10-year-old son who is 10 years old. I have a 10-year-old son who\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:34:33.361229Z","iopub.execute_input":"2025-01-03T03:34:33.361530Z","iopub.status.idle":"2025-01-03T03:34:37.168818Z","shell.execute_reply.started":"2025-01-03T03:34:33.361494Z","shell.execute_reply":"2025-01-03T03:34:37.167921Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install rouge_score\n!pip install tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:34:37.170086Z","iopub.execute_input":"2025-01-03T03:34:37.170453Z","iopub.status.idle":"2025-01-03T03:34:45.770192Z","shell.execute_reply.started":"2025-01-03T03:34:37.170418Z","shell.execute_reply":"2025-01-03T03:34:45.769039Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=977f92bc851d771aced93648f69604adf0cfb648e3c54579f46a4fa8406d4cfc\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Import the evaluate library\nimport evaluate\n\n# Initialize BLEU and ROUGE metrics\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:34:45.771406Z","iopub.execute_input":"2025-01-03T03:34:45.771764Z","iopub.status.idle":"2025-01-03T03:34:47.373005Z","shell.execute_reply.started":"2025-01-03T03:34:45.771732Z","shell.execute_reply":"2025-01-03T03:34:47.372378Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c3b830c35754b02a40e12c3f8baebfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc84c05603614104ab8ded718d27e46f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06069855a3e043d5b35f604292ad3951"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54022109e9c2400c9b050cf2cb991288"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Prepare the model for inference\nFastLanguageModel.for_inference(student_model)  # Enable native 2x faster inference\n\n# Function to generate predictions and compute metrics\ndef evaluate_model(model, tokenizer, eval_dataset, batch_size=1):\n    from torch.utils.data import DataLoader\n    from tqdm import tqdm  # Import tqdm for progress bar\n\n    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n    predictions = []\n    references = []\n\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            instructions = batch['instruction']\n            inputs_text = batch['context']\n            outputs_text = batch['response']\n\n            batch_messages = []\n            for instruction, input_text in zip(instructions, inputs_text):\n                message = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Instruction: {instruction}\\nInput: {input_text}\"\n                    }\n                ]\n                batch_messages.append(message)\n\n            # Apply chat template and tokenize\n            inputs = tokenizer.apply_chat_template(\n                batch_messages,\n                tokenize=True,\n                add_generation_prompt=True,  # Must add for generation\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=2048,\n            ).to(\"cuda\")\n\n            # Compute attention mask\n            attention_mask = inputs.ne(tokenizer.pad_token_id).long()\n\n            # Generate outputs from the model\n            outputs = model.generate(\n                input_ids=inputs,\n                attention_mask=attention_mask,\n                max_new_tokens=128,\n                temperature=0.8,  # Slightly lower for more coherence\n                top_p=0.9,       # Increased for better diversity\n            )\n\n            # Get the length of the inputs\n            input_length = inputs.shape[1]\n\n            # Decode the generated text\n            generated_tokens = outputs[:, input_length:]\n            decoded_outputs = tokenizer.batch_decode(\n                generated_tokens, skip_special_tokens=True\n            )\n\n            # Append the generated text to predictions\n            predictions.extend(decoded_outputs)\n            # Append the reference outputs\n            references.extend(outputs_text)\n\n    # Compute BLEU and ROUGE scores\n    bleu_score = bleu_metric.compute(predictions=predictions, references=references)\n    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n\n    print(f\"BLEU score: {bleu_score}\")\n    print(f\"ROUGE score: {rouge_score}\")\n\n# Evaluate the model\nevaluate_model(student_model, student_tokenizer, eval_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:34:47.373845Z","iopub.execute_input":"2025-01-03T03:34:47.374055Z","iopub.status.idle":"2025-01-03T05:43:35.213606Z","shell.execute_reply.started":"2025-01-03T03:34:47.374037Z","shell.execute_reply":"2025-01-03T05:43:35.212870Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 1502/1502 [2:08:40<00:00,  5.14s/it] \n","output_type":"stream"},{"name":"stdout","text":"BLEU score: {'bleu': 0.06980420610589691, 'precisions': [0.22294559172403064, 0.07786688493109717, 0.04381067671208875, 0.031217255331839503], 'brevity_penalty': 1.0, 'length_ratio': 1.2233799600342563, 'translation_length': 128565, 'reference_length': 105090}\nROUGE score: {'rouge1': 0.24162945115807385, 'rouge2': 0.1000732247680706, 'rougeL': 0.18976199534204646, 'rougeLsum': 0.19936033274048165}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}