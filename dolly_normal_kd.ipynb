{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth\n!pip install datasets transformers accelerate einops bitsandbytes trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:38:02.989900Z","iopub.execute_input":"2025-01-03T03:38:02.990211Z","iopub.status.idle":"2025-01-03T03:41:09.607418Z","shell.execute_reply.started":"2025-01-03T03:38:02.990187Z","shell.execute_reply":"2025-01-03T03:41:09.606240Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:41:09.608842Z","iopub.execute_input":"2025-01-03T03:41:09.609077Z","iopub.status.idle":"2025-01-03T03:41:26.631623Z","shell.execute_reply.started":"2025-01-03T03:41:09.609057Z","shell.execute_reply":"2025-01-03T03:41:26.630738Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"################################################################\n# 1) Load the TEACHER (frozen)\n################################################################\nteacher_max_seq_length = 2048 \nteacher_dtype = None           # Auto-detect dtype\nteacher_load_in_4bit = True    # 4bit quant for memory savings\n\nteacher_model, teacher_tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/SmolLM2-1.7B-Instruct\",\n    max_seq_length=teacher_max_seq_length,\n    dtype=teacher_dtype,\n    load_in_4bit=teacher_load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:41:26.633513Z","iopub.execute_input":"2025-01-03T03:41:26.633815Z","iopub.status.idle":"2025-01-03T03:41:43.664234Z","shell.execute_reply.started":"2025-01-03T03:41:26.633793Z","shell.execute_reply":"2025-01-03T03:41:43.663301Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12232111f6f149e9aa5cd1a51b3d9e5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aa56791ef6d455a98932ff3aa47c97c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/4.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2dc27c7a36c491a85335073e7faa2b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3abced514bbb4b19841a65305a04f991"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79e7c0f647b349e7bbdc654803145980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"580179b3c3eb401eb4be9fbecc7e1b16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/423 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afc6745a94534070bec2878e57ac7ab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"908d4d1e30014324b5ed22362581928e"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Freeze teacher weights to avoid training them\nteacher_model.eval()\nfor param in teacher_model.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:41:43.665827Z","iopub.execute_input":"2025-01-03T03:41:43.666176Z","iopub.status.idle":"2025-01-03T03:41:43.674530Z","shell.execute_reply.started":"2025-01-03T03:41:43.666145Z","shell.execute_reply":"2025-01-03T03:41:43.673668Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"################################################################\n# 2) Load the STUDENT (with LoRA)\n################################################################\nstudent_max_seq_length = 2048\nstudent_dtype = None            # Auto-detect dtype\nstudent_load_in_4bit = True     # 4bit quant for memory savings\n\nstudent_model, student_tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/SmolLM2-135M-Instruct\",\n    max_seq_length=student_max_seq_length,\n    dtype=student_dtype,\n    load_in_4bit=student_load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:41:43.675559Z","iopub.execute_input":"2025-01-03T03:41:43.675916Z","iopub.status.idle":"2025-01-03T03:41:57.301568Z","shell.execute_reply.started":"2025-01-03T03:41:43.675884Z","shell.execute_reply":"2025-01-03T03:41:57.300871Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02ac9e44cf1a4ac181e91f6540e77d92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ec618739318482c96d2d3873c9d94b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa92b937ba1c47cebe200c095651caf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c655c8cc32a411da23c04e9e3bb8443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f82393b52806445f90046e54ed8e2c5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac2d154df65f4953b80c1c2c7f37cb6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/423 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c0ba8a4cba4ea4a3d3df56c588b891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93c4f26a175844d5b3fb356b3c2c504a"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Apply LoRA to the student\nstudent_model = FastLanguageModel.get_peft_model(\n    student_model,\n    r=16,\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\", \n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:41:57.302336Z","iopub.execute_input":"2025-01-03T03:41:57.302546Z","iopub.status.idle":"2025-01-03T03:42:02.424471Z","shell.execute_reply.started":"2025-01-03T03:41:57.302529Z","shell.execute_reply":"2025-01-03T03:42:02.423696Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2024.12.12 patched 30 layers with 30 QKV layers, 30 O layers and 30 MLP layers.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"################################################################\n# 3) Prepare the Prompt Template\n################################################################\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, \npaired with an input that provides further context. \nWrite a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = student_tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"context\"]\n    outputs      = examples[\"response\"]\n    texts = []\n    for instr, inp, outp in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instr, inp, outp)\n        # Add EOS to ensure generation terminates\n        text += EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n################################################################\n# 4) Load the Alpaca Dataset & Apply Prompt-Formatting\n################################################################\n# Load and split the dataset\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n\n# Split dataset into training and validation sets\nfrom datasets import Dataset\n\n# Split dataset into training and validation sets\ndataset = dataset.train_test_split(test_size=0.1, seed=42)\ntrain_dataset = dataset['train']\neval_dataset = dataset['test']\n\n# Ensure test set has exactly 1000 entries without shuffling\n# eval_dataset = (eval_dataset.select(range(1000)))\n\n# Apply formatting to both datasets\ntrain_dataset = train_dataset.map(formatting_prompts_func, batched=True)\neval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:42:02.425326Z","iopub.execute_input":"2025-01-03T03:42:02.425592Z","iopub.status.idle":"2025-01-03T03:42:04.195761Z","shell.execute_reply.started":"2025-01-03T03:42:02.425571Z","shell.execute_reply":"2025-01-03T03:42:04.194723Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd72cc1b10434de098370d994d447b59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"901b9c6a81ad4f3e9599101de8cc5306"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0c9dbe7fb04d0a86544db888e6f8da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13509 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a24392e19b24a7d9a8646cadc826b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1502 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"819100f330d34b9e80f9a4d56daddc5e"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import torch.nn as nn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:42:04.198216Z","iopub.execute_input":"2025-01-03T03:42:04.198463Z","iopub.status.idle":"2025-01-03T03:42:04.202423Z","shell.execute_reply.started":"2025-01-03T03:42:04.198443Z","shell.execute_reply":"2025-01-03T03:42:04.201470Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"###############################################################\n# 4) Distillation Trainer\n###############################################################\nclass DistillationTrainer(SFTTrainer):\n    def __init__(\n        self,\n        teacher_model,\n        alpha=0.5,       # weight for dataset CE\n        temperature=1.0, # teacher distribution softening\n        *args, \n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n        self.alpha = alpha\n        self.temperature = temperature\n        \n        # Freeze teacher\n        self.teacher_model.eval()\n        for p in self.teacher_model.parameters():\n            p.requires_grad = False\n\n        # We'll manually compute CE, so define a suitable loss\n        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n\n    # # Accept **kwargs to handle extra unsloth arguments like num_items_in_batch\n    # def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n    #     # We'll do forward pass on student WITHOUT labels to ensure we get `logits`\n    #     student_out = model(\n    #         input_ids=inputs[\"input_ids\"],\n    #         attention_mask=inputs[\"attention_mask\"],\n    #     )\n    #     student_logits = student_out.logits  # shape (batch, seq_len, vocab_size)\n\n    #     # Manually compute cross-entropy against the known labels\n    #     labels = inputs[\"labels\"]  # shape (batch, seq_len)\n    #     ce_loss = self.loss_fct(\n    #         student_logits.view(-1, student_logits.size(-1)),\n    #         labels.view(-1)\n    #     )\n\n    #     # Teacher forward pass\n    #     with torch.no_grad():\n    #         teacher_out = self.teacher_model(\n    #             input_ids=inputs[\"input_ids\"],\n    #             attention_mask=inputs[\"attention_mask\"],\n    #         )\n    #         teacher_logits = teacher_out.logits\n\n    #     # Build a valid mask to ignore label=-100 tokens\n    #     valid_mask = labels.ne(-100)\n\n    #     # Student + teacher distributions\n    #     student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n    #     teacher_probs     = F.softmax(teacher_logits / self.temperature, dim=-1)\n\n    #     # Filter out invalid tokens\n    #     student_log_probs = student_log_probs[valid_mask]\n    #     teacher_probs     = teacher_probs[valid_mask]\n\n    #     # KL Divergence\n    #     kl_loss = F.kl_div(\n    #         student_log_probs,\n    #         teacher_probs,\n    #         reduction=\"batchmean\"\n    #     )\n\n    #     # Combine\n    #     loss = self.alpha * ce_loss + (1 - self.alpha) * (self.temperature**2) * kl_loss\n    #     return (loss, student_out) if return_outputs else loss\n\ndef compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n    labels = inputs[\"labels\"]\n\n    # 1) FORWARD PASS *without* labels\n    student_out = model(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        # remove labels=labels here\n    )\n    student_logits = student_out.logits  # No longer None!\n\n    # 2) Manually compute CE\n    ce_loss = self.loss_fct(\n        student_logits.view(-1, student_logits.size(-1)),\n        labels.view(-1),\n    )\n\n    # 3) Teacher pass\n    with torch.no_grad():\n        teacher_out = self.teacher_model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n        )\n        teacher_logits = teacher_out.logits\n\n    # Apply perception with dim=1 (normalizing across the sequence dimension)\n    # student_logits = perception(student_logits, dim=1)\n    # teacher_logits = perception(teacher_logits, dim=1)\n\n    # 4) KD portion: apply mask for label=-100 tokens, compute KL, combine\n    valid_mask = labels.ne(-100)\n    student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n    teacher_probs     = F.softmax(teacher_logits / self.temperature, dim=-1)\n    student_log_probs = student_log_probs[valid_mask]\n    teacher_probs     = teacher_probs[valid_mask]\n    kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction=\"batchmean\")\n\n    # 5) Final distillation loss\n    loss = self.alpha * ce_loss + (1 - self.alpha) * (self.temperature ** 2) * kl_loss\n    return (loss, student_out) if return_outputs else loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:42:04.203826Z","iopub.execute_input":"2025-01-03T03:42:04.204063Z","iopub.status.idle":"2025-01-03T03:42:04.303147Z","shell.execute_reply.started":"2025-01-03T03:42:04.204044Z","shell.execute_reply":"2025-01-03T03:42:04.302086Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"################################################################\n# 6) Set Up TrainingArguments & DistillationTrainer\n################################################################\ntrain_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=60,               # or use num_train_epochs=1 for a full epoch\n    learning_rate=2e-4,\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"distilled_outputs\",\n    report_to=\"none\",  # or \"wandb\", \"tensorboard\", etc.\n)\n\ntrainer = DistillationTrainer(\n    teacher_model=teacher_model,\n    alpha=0.5,           # how much to weigh the ground-truth CE vs. teacher KL\n    temperature=1.0,     # if you need a \"softer\" teacher distribution, raise T\n    model=student_model,\n    tokenizer=student_tokenizer,\n    train_dataset=train_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=student_max_seq_length,\n    dataset_num_proc=2,\n    packing=False,\n    args=train_args,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:42:04.304130Z","iopub.execute_input":"2025-01-03T03:42:04.304461Z","iopub.status.idle":"2025-01-03T03:42:11.136980Z","shell.execute_reply.started":"2025-01-03T03:42:04.304427Z","shell.execute_reply":"2025-01-03T03:42:11.135987Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/13509 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fde086efe174947a5a9e6b24b87a9b0"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"################################################################\n# 7) Launch Distillation\n################################################################\ntrainer_stats = trainer.train()\nprint(trainer_stats)\n\n################################################################\n# 8) (Optional) Save or Push Your LoRA Adapters\n################################################################\n# student_model.save_pretrained(\"my_distilled_student_lora\")\n# student_model.push_to_hub(\"myusername/SmolLM2-135M-distilled-lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:42:11.138090Z","iopub.execute_input":"2025-01-03T03:42:11.138432Z","iopub.status.idle":"2025-01-03T03:44:05.181672Z","shell.execute_reply.started":"2025-01-03T03:42:11.138398Z","shell.execute_reply":"2025-01-03T03:44:05.180941Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 13,509 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 4,884,480\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 01:38, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.974700</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.724000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.942900</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.572700</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.084600</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.914000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.472200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.741100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>3.007000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.719400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.390700</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.239000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.657200</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.830300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.652500</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.399600</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2.656300</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>2.482200</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.796600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.873600</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>2.466800</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>2.651400</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>2.497400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.224300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.443700</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>2.402400</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>2.539100</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>2.180600</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>2.276500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.886100</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>2.454900</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>2.156800</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>2.201400</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>2.314200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>2.322600</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>2.051700</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>2.157200</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>2.243600</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>2.289000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.521900</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>2.890900</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.966800</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>2.038200</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>2.481200</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.821500</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>2.165600</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>2.042500</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>1.826700</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>2.024200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.813000</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>1.958500</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.827700</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>2.210800</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>2.164300</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.957200</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>1.934400</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.472400</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>1.695000</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>2.599900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.091900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"TrainOutput(global_step=60, training_loss=2.339911820491155, metrics={'train_runtime': 110.8204, 'train_samples_per_second': 4.331, 'train_steps_per_second': 0.541, 'total_flos': 107089659007488.0, 'train_loss': 2.339911820491155, 'epoch': 0.03552923760177646})\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"################################################################\n# 7) (OPTIONAL) INFERENCE EXAMPLE\n################################################################\n# After training, let's enable 2x faster inference with Unsloth\nFastLanguageModel.for_inference(student_model)  # Speed optimization\n\n# We'll create a prompt using the same alpaca_prompt format\n# but we'll leave the \"output\" blank so the model can generate.\n\ninference_text = alpaca_prompt.format(\n    \"who am  i?\",  # instruction\n    \"\",  # context\n    \"\"   # response left blank for generation\n)\n\ninputs = student_tokenizer([inference_text], return_tensors=\"pt\").to(\"cuda\")\n\n# Use Hugging Face's TextStreamer to see the live output tokens\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(student_tokenizer)\n\n# Generate up to 128 new tokens from the student model\n_ = student_model.generate(\n    **inputs,\n    streamer=text_streamer,\n    max_new_tokens=128,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:44:05.182493Z","iopub.execute_input":"2025-01-03T03:44:05.182786Z","iopub.status.idle":"2025-01-03T03:44:12.169366Z","shell.execute_reply.started":"2025-01-03T03:44:05.182750Z","shell.execute_reply":"2025-01-03T03:44:12.168752Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, \npaired with an input that provides further context. \nWrite a response that appropriately completes the request.\n\n### Instruction:\nwho am  i?\n\n### Input:\n\n\n### Response:\n\nI am a 25-year-old male, born in the United States, who is from a small town in the Midwest. I have a family of 10 children, and I have a 10-year-old son who is 10 years old. I have a 10-year-old daughter who is 10 years old. I have a 10-year-old son who is 10 years old. I have a 10-year-old son who is 10 years old. I have a 10-year-old son who\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:44:12.170157Z","iopub.execute_input":"2025-01-03T03:44:12.170474Z","iopub.status.idle":"2025-01-03T03:44:16.033423Z","shell.execute_reply.started":"2025-01-03T03:44:12.170442Z","shell.execute_reply":"2025-01-03T03:44:16.032476Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install rouge_score\n!pip install tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:44:16.034464Z","iopub.execute_input":"2025-01-03T03:44:16.034721Z","iopub.status.idle":"2025-01-03T03:44:24.860691Z","shell.execute_reply.started":"2025-01-03T03:44:16.034686Z","shell.execute_reply":"2025-01-03T03:44:24.859603Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=d085e0326f47f0de2d4272127932724f0a6443303e452befa6bfc38d29a24faf\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Import the evaluate library\nimport evaluate\n\n# Initialize BLEU and ROUGE metrics\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:44:24.861893Z","iopub.execute_input":"2025-01-03T03:44:24.862219Z","iopub.status.idle":"2025-01-03T03:44:26.518165Z","shell.execute_reply.started":"2025-01-03T03:44:24.862187Z","shell.execute_reply":"2025-01-03T03:44:26.517494Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c6d3c991df84d16b04866ec45659e99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa113fad89a24539883ca0e92350a9f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73fb8b912c94476e93cbc6cf58c18260"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa0e37e6b1940beb6021d0895822016"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Prepare the model for inference\nFastLanguageModel.for_inference(student_model)  # Enable native 2x faster inference\n\n# Function to generate predictions and compute metrics\ndef evaluate_model(model, tokenizer, eval_dataset, batch_size=1):\n    from torch.utils.data import DataLoader\n    from tqdm import tqdm  # Import tqdm for progress bar\n\n    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n    predictions = []\n    references = []\n\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            instructions = batch['instruction']\n            inputs_text = batch['context']\n            outputs_text = batch['response']\n\n            batch_messages = []\n            for instruction, input_text in zip(instructions, inputs_text):\n                message = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Instruction: {instruction}\\nInput: {input_text}\"\n                    }\n                ]\n                batch_messages.append(message)\n\n            # Apply chat template and tokenize\n            inputs = tokenizer.apply_chat_template(\n                batch_messages,\n                tokenize=True,\n                add_generation_prompt=True,  # Must add for generation\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=2048,\n            ).to(\"cuda\")\n\n            # Compute attention mask\n            attention_mask = inputs.ne(tokenizer.pad_token_id).long()\n\n            # Generate outputs from the model\n            outputs = model.generate(\n                input_ids=inputs,\n                attention_mask=attention_mask,\n                max_new_tokens=128,\n                temperature=0.8,  # Slightly lower for more coherence\n                top_p=0.9,       # Increased for better diversity\n            )\n\n            # Get the length of the inputs\n            input_length = inputs.shape[1]\n\n            # Decode the generated text\n            generated_tokens = outputs[:, input_length:]\n            decoded_outputs = tokenizer.batch_decode(\n                generated_tokens, skip_special_tokens=True\n            )\n\n            # Append the generated text to predictions\n            predictions.extend(decoded_outputs)\n            # Append the reference outputs\n            references.extend(outputs_text)\n\n    # Compute BLEU and ROUGE scores\n    bleu_score = bleu_metric.compute(predictions=predictions, references=references)\n    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n\n    print(f\"BLEU score: {bleu_score}\")\n    print(f\"ROUGE score: {rouge_score}\")\n\n# Evaluate the model\nevaluate_model(student_model, student_tokenizer, eval_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:44:26.518971Z","iopub.execute_input":"2025-01-03T03:44:26.519267Z","iopub.status.idle":"2025-01-03T05:55:29.409021Z","shell.execute_reply.started":"2025-01-03T03:44:26.519237Z","shell.execute_reply":"2025-01-03T05:55:29.408185Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1502/1502 [2:10:55<00:00,  5.23s/it] \n","output_type":"stream"},{"name":"stdout","text":"BLEU score: {'bleu': 0.06949902997802422, 'precisions': [0.22198690498355364, 0.07743206543068398, 0.04359740115327795, 0.03113193897400447], 'brevity_penalty': 1.0, 'length_ratio': 1.2266057664858692, 'translation_length': 128904, 'reference_length': 105090}\nROUGE score: {'rouge1': 0.24114172608991696, 'rouge2': 0.09963050558902084, 'rougeL': 0.18991189825217736, 'rougeLsum': 0.19986230291352416}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}